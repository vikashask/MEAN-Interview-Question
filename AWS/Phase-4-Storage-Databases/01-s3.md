# S3 (Simple Storage Service)

## Basic S3 Operations

```bash
# Create bucket
aws s3 mb s3://my-bucket

# List buckets
aws s3 ls

# List objects in bucket
aws s3 ls s3://my-bucket
aws s3 ls s3://my-bucket/folder/ --recursive

# Upload file
aws s3 cp file.txt s3://my-bucket/
aws s3 cp file.txt s3://my-bucket/ --storage-class INTELLIGENT_TIERING

# Download file
aws s3 cp s3://my-bucket/file.txt ./

# Sync directory
aws s3 sync ./local-folder s3://my-bucket/remote-folder/
aws s3 sync s3://my-bucket/remote-folder/ ./local-folder

# Delete file
aws s3 rm s3://my-bucket/file.txt

# Delete bucket
aws s3 rb s3://my-bucket --force
```

## S3 with SDK

```javascript
import {
  S3Client,
  PutObjectCommand,
  GetObjectCommand,
  ListObjectsV2Command,
  DeleteObjectCommand,
} from "@aws-sdk/client-s3";
import { getSignedUrl } from "@aws-sdk/s3-request-presigner";
import { Upload } from "@aws-sdk/lib-storage";
import fs from "fs";

const s3 = new S3Client({ region: "us-east-1" });

// Upload file
const uploadFile = async (bucket, key, filePath) => {
  const fileStream = fs.createReadStream(filePath);
  const command = new PutObjectCommand({
    Bucket: bucket,
    Key: key,
    Body: fileStream,
    ContentType: "application/pdf",
    Metadata: {
      uploadedBy: "user123",
      uploadDate: new Date().toISOString(),
    },
  });

  await s3.send(command);
};

// Upload large file with multipart
const uploadLargeFile = async (bucket, key, filePath) => {
  const fileStream = fs.createReadStream(filePath);
  const upload = new Upload({
    client: s3,
    params: {
      Bucket: bucket,
      Key: key,
      Body: fileStream,
    },
    queueSize: 4,
    partSize: 5 * 1024 * 1024, // 5MB parts
  });

  upload.on("httpUploadProgress", (progress) => {
    console.log(`Uploaded ${progress.loaded} of ${progress.total} bytes`);
  });

  await upload.done();
};

// Download file
const downloadFile = async (bucket, key) => {
  const command = new GetObjectCommand({
    Bucket: bucket,
    Key: key,
  });

  const response = await s3.send(command);
  const content = await response.Body.transformToString();
  return content;
};

// List objects
const listObjects = async (bucket, prefix = "") => {
  const command = new ListObjectsV2Command({
    Bucket: bucket,
    Prefix: prefix,
    MaxKeys: 1000,
  });

  const response = await s3.send(command);
  return response.Contents;
};

// Delete object
const deleteObject = async (bucket, key) => {
  const command = new DeleteObjectCommand({
    Bucket: bucket,
    Key: key,
  });

  await s3.send(command);
};

// Generate presigned URL
const getPresignedUrl = async (bucket, key, expiresIn = 3600) => {
  const command = new GetObjectCommand({
    Bucket: bucket,
    Key: key,
  });

  const url = await getSignedUrl(s3, command, { expiresIn });
  return url;
};
```

## Bucket Policies

```json
// Public read access
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-bucket/*"
    }
  ]
}

// Restrict to specific IP
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-bucket/*",
      "Condition": {
        "IpAddress": {
          "aws:SourceIp": "203.0.113.0/24"
        }
      }
    }
  ]
}

// CloudFront access only
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity ABC123"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-bucket/*"
    }
  ]
}

// Cross-account access
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:root"
      },
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::my-bucket/*"
    }
  ]
}
```

## Lifecycle Policies

```json
{
  "Rules": [
    {
      "Id": "MoveToIA",
      "Status": "Enabled",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        },
        {
          "Days": 365,
          "StorageClass": "DEEP_ARCHIVE"
        }
      ]
    },
    {
      "Id": "DeleteOldVersions",
      "Status": "Enabled",
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 30
      }
    },
    {
      "Id": "DeleteIncompleteUploads",
      "Status": "Enabled",
      "AbortIncompleteMultipartUpload": {
        "DaysAfterInitiation": 7
      }
    },
    {
      "Id": "DeleteTempFiles",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "temp/"
      },
      "Expiration": {
        "Days": 7
      }
    }
  ]
}
```

## S3 Versioning

```bash
# Enable versioning
aws s3api put-bucket-versioning \
  --bucket my-bucket \
  --versioning-configuration Status=Enabled

# List versions
aws s3api list-object-versions --bucket my-bucket

# Get specific version
aws s3api get-object \
  --bucket my-bucket \
  --key file.txt \
  --version-id version-id \
  output.txt

# Delete specific version
aws s3api delete-object \
  --bucket my-bucket \
  --key file.txt \
  --version-id version-id
```

## S3 Replication

```json
// Cross-region replication configuration
{
  "Role": "arn:aws:iam::123456789012:role/s3-replication-role",
  "Rules": [
    {
      "Status": "Enabled",
      "Priority": 1,
      "DeleteMarkerReplication": {
        "Status": "Enabled"
      },
      "Filter": {
        "Prefix": "documents/"
      },
      "Destination": {
        "Bucket": "arn:aws:s3:::my-replica-bucket",
        "ReplicationTime": {
          "Status": "Enabled",
          "Time": {
            "Minutes": 15
          }
        },
        "Metrics": {
          "Status": "Enabled",
          "EventThreshold": {
            "Minutes": 15
          }
        }
      }
    }
  ]
}
```

## S3 Event Notifications

```json
// Lambda notification configuration
{
  "LambdaFunctionConfigurations": [
    {
      "LambdaFunctionArn": "arn:aws:lambda:us-east-1:123456789012:function:ProcessImage",
      "Events": ["s3:ObjectCreated:*"],
      "Filter": {
        "Key": {
          "FilterRules": [
            {
              "Name": "prefix",
              "Value": "images/"
            },
            {
              "Name": "suffix",
              "Value": ".jpg"
            }
          ]
        }
      }
    }
  ],
  "QueueConfigurations": [
    {
      "QueueArn": "arn:aws:sqs:us-east-1:123456789012:my-queue",
      "Events": ["s3:ObjectRemoved:*"]
    }
  ]
}
```

## S3 Static Website Hosting

```bash
# Enable website hosting
aws s3 website s3://my-bucket \
  --index-document index.html \
  --error-document error.html

# Upload website files
aws s3 sync ./dist s3://my-bucket --delete

# Set public read policy
aws s3api put-bucket-policy \
  --bucket my-bucket \
  --policy file://public-policy.json
```

## S3 Performance Optimization

```javascript
// Parallel uploads for better performance
import { Upload } from "@aws-sdk/lib-storage";

const parallelUpload = async (bucket, files) => {
  const uploads = files.map((file) => {
    const upload = new Upload({
      client: s3,
      params: {
        Bucket: bucket,
        Key: file.name,
        Body: file.stream,
      },
      queueSize: 4,
      partSize: 10 * 1024 * 1024, // 10MB parts
    });

    return upload.done();
  });

  await Promise.all(uploads);
};

// Prefix distribution for high request rates
// Instead of: user123/file1.txt, user123/file2.txt
// Use:        a/user123/file1.txt, b/user123/file2.txt
const generateKey = (userId, fileName) => {
  const hash = userId.charCodeAt(0) % 10;
  return `${hash}/${userId}/${fileName}`;
};
```

## S3 Select

```javascript
import { SelectObjectContentCommand } from "@aws-sdk/client-s3";

// Query CSV file
const queryCSV = async (bucket, key) => {
  const command = new SelectObjectContentCommand({
    Bucket: bucket,
    Key: key,
    ExpressionType: "SQL",
    Expression: "SELECT * FROM s3object s WHERE s.age > 25",
    InputSerialization: {
      CSV: {
        FileHeaderInfo: "USE",
        RecordDelimiter: "\n",
        FieldDelimiter: ",",
      },
    },
    OutputSerialization: {
      JSON: {
        RecordDelimiter: "\n",
      },
    },
  });

  const response = await s3.send(command);

  let result = "";
  for await (const event of response.Payload) {
    if (event.Records) {
      result += event.Records.Payload.toString();
    }
  }

  return result;
};
```
